Variational Auto-Encoders (VAEs) are presented as a versatile generative framework with applications in data compression, data generation, noise reduction, anomaly detection, and missing data imputation, demonstrated on MNIST with practical PyTorch examples. The article explains the encoderâ€“decoder architecture, the probabilistic latent space, and the reparameterization trick that enables end-to-end training. It contrasts VAEs with traditional autoencoders and describes a loss combining reconstruction error with KL divergence. Practical sections show compressing images into a compact latent representation, generating new digits, denoising, anomaly detection (including out-of-distribution letters), and imputation, plus a GAN comparison and future directions.